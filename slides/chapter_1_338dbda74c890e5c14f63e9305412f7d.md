---
title: Insert title here
key: 338dbda74c890e5c14f63e9305412f7d

---
## Introduction to RNN and Basic Assumptions of various models

```yaml
type: "TitleSlide"
key: "00298cd64c"
```

`@lower_third`

name: Aakash Moghariya
title: Machine Learning Engineer


`@script`
In previous video lecture you learned about sequence data types. We also explored real world examples of such dataset. In this video we are going to explore some of the technique that can be used to process such data and their limitations. Eventually, we will learn about state of the art neural network called Recurrent neural networks for sequence data processing.


---
## Sequence Data Processing Intuition

```yaml
type: "FullSlide"
key: "5f204109d5"
```

`@part1`
Best working example of state of art sequence data processing mechanism is human brain.

Human brain processes sequence data by

- Persisting previous information such as words, sentences, context.
- Given context and previous experience tries to predict foreseeable pattern.


`@script`
Many difficult and complex real world engineering problems can be solved using nature as inspiration. For example, first airplanes designed by Wright brothers took bird and their flight as example. Similarly, for sequence data processing we can also follow similar intuition of taking best working example from nature for base design. Best working example of state of art sequence data processing mechanism is human brain. Human brain processing sequence data by Persisting previous information such as words, sentences, context. Meaning human brain stores and understands each word before processing the next word. And it uses the previous word to understand the next word. Or in case of bigger problem, human brain tries to remember previous sentences or context to understand the whole context of the text or situation. Often times, human brain given context or previous experience tries to predict foreseeable pattern. And if the brain encounters different outcome than expected or predicted than it often gets shocked or surprised!


---
## Limitation of Convoluted Neural Network

```yaml
type: "FullSlide"
key: "2baf7bf0dd"
```

`@part1`
Sequence data processing with CNN and traditional neural networks has fundamental flaws.

1. CNN assumes all datapoint are independent and previous datapoint doesn't affect results of current datapoint.
2. Most CNN for Natural Language Processing problem, including state of the art models, contains filter with maximum size of 5 or 7.
3. Accepts fixed size inputs and produces fixed size output.

Point 2 tried to address limitation of 1 in certain way but point 1 still holds true despite the technique.


`@script`
Traditional neural network and convoluted neural networks have became state of the art for variety of applications such as image processing and sentiment analysis. Hence, it is natural to implement these as base model to evaluate performance on problems like Language modeling or other complex natural language processing task. However, these models have few fundamental flaws. One of the major assumption of CNN model is that it assumes all datapoint are independent and previous datapoint doesn't affect results of current datapoint. However, as we learned in earlier slides, for sequence data processing knowing previous state or context is very essential to predict the existing state. Hence, these models are not better suited for such problems. However, CNN did try to tackle the problem of knowing previous state using different size filter for convolution for extracting information/context. But this mechanism is limited to one single example only. And this can work great when it comes to problems like sentiment analysis, but in problems like generating text sequences, it would not be useful. Lastly, CNN and other traditional neural networks tend to accept fixed size input and produce fixed size output which not the case when it comes to sequence data.


---
## Introduction to Recurrent Neural Networks

```yaml
type: "FullSlide"
key: "2c953d9da1"
```

`@part1`
Recurrent neural network tries to emit the behavior or human brain by incorporating previous information into computation of existing state. Image below illustrates simple Recurrent neural network.

![RNN-Model](https://i.imgur.com/OqXYQX5.png)


`@script`
Introduction to Recurrent Neural Networks


---
## Introduction to Recurrent Neural Network Continued

```yaml
type: "FullSlide"
key: "c1d67f508d"
```

`@part1`
In simple terms, Recurrent neural networks are chain of repeated (recurrent) small networks that perform similar operations. Hence they are called "Recurrent" neural networks. 

Recurrent neural networks can handle variable size input data and output data and persist information unlike traditional neural networks.

Because of the memory persisting nature of recurrent neural network, they are state of the art for variety of application in Natural Language processing such as 
- Language Modeling
- Generating Text
- Machine Translation
- Chatbots


`@script`
Define Recurrent in RNN and Applications of RNN.


---
## Simple RNN using Keras

```yaml
type: "FullSlide"
key: "8b3c2ba17d"
```

`@part1`
```
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import Embedding
from keras.layers import LSTM

max_features = 1024

model = Sequential()
model.add(Embedding(max_features, output_dim=256))
model.add(LSTM(128))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

model.fit(x_train, y_train, batch_size=16, epochs=10)
score = model.evaluate(x_test, y_test, batch_size=16)```


`@script`
Code for simple RNN model implementation.


---
## Conclusion

```yaml
type: "FinalSlide"
key: "4ce843e7d4"
```

`@script`
Conclusion and future topics

