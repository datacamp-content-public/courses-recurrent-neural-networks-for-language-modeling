---
title: Insert title here
key: 338dbda74c890e5c14f63e9305412f7d

---
## Introduction to RNN and Basic Assumptions of various models

```yaml
type: "TitleSlide"
key: "00298cd64c"
```

`@lower_third`

name: Aakash Moghariya
title: Machine Learning Engineer


`@script`
Learn the term “recurrent”. View one basic architecture of the RNN model.
Understand main differences between underlying assumptions and compare how they differ from each other.


---
## Sequence Data Processing Intuition

```yaml
type: "FullSlide"
key: "5f204109d5"
```

`@part1`
Best working example of state of art sequence data processing mechanism is human brain.

Human brain processes sequence data by

- Persisting previous information such as words, sentences, context.
- Given context and previous experience tries to predict foreseeable pattern.


`@script`
Natural inspiration for Sequence Data Processing


---
## Limitation of Convoluted Neural Network

```yaml
type: "FullSlide"
key: "2baf7bf0dd"
```

`@part1`
Sequence data processing with CNN and traditional neural networks has fundamental flaws.

1. CNN assumes all datapoint are independent and previous datapoint doesn't affect results of current datapoint.
2. Most CNN for Natural Language Processing problem, including state of the art models, contains filter with maximum size of 5 or 7.
3. Accepts fixed size inputs and produces fixed size output.

Point 2 tried to address limitation of 1 in certain way but point 1 still holds true despite the technique.


`@script`
Teaches student limitations of traditional approaches such as convoluted neural network to process sequence data. It also states assumptions of CNN.


---
## Introduction to Recurrent Neural Networks

```yaml
type: "FullSlide"
key: "2c953d9da1"
```

`@part1`
Recurrent neural network tries to emit the behavior or human brain by incorporating previous information into computation of existing state. Image below illustrates simple Recurrent neural network.

![RNN-Model](https://i.imgur.com/OqXYQX5.png)


`@script`
Introduction to Recurrent Neural Networks


---
## Introduction to Recurrent Neural Network Continued

```yaml
type: "FullSlide"
key: "c1d67f508d"
```

`@part1`
In simple terms, Recurrent neural networks are chain of repeated (recurrent) small networks that perform similar operations. Hence they are called "Recurrent" neural networks. 

Recurrent neural networks can handle variable size input data and output data and persist information unlike traditional neural networks.

Because of the memory persisting nature of recurrent neural network, they are state of the art for variety of application in Natural Language processing such as 
- Language Modeling
- Generating Text
- Machine Translation
- Chatbots


`@script`
Define Recurrent in RNN and Applications of RNN.


---
## Simple RNN using Keras

```yaml
type: "FullSlide"
key: "8b3c2ba17d"
```

`@part1`
```
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import Embedding
from keras.layers import LSTM

max_features = 1024

model = Sequential()
model.add(Embedding(max_features, output_dim=256))
model.add(LSTM(128))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

model.fit(x_train, y_train, batch_size=16, epochs=10)
score = model.evaluate(x_test, y_test, batch_size=16)```


`@script`
Code for simple RNN model implementation.


---
## Conclusion

```yaml
type: "FinalSlide"
key: "4ce843e7d4"
```

`@script`
Conclusion and future topics

