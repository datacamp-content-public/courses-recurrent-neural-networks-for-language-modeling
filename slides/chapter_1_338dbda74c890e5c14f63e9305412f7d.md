---
title: Insert title here
key: 338dbda74c890e5c14f63e9305412f7d

---
## Introduction to RNN and Basic Assumptions of various models

```yaml
type: "TitleSlide"
key: "00298cd64c"
```

`@lower_third`

name: Aakash Moghariya
title: Machine Learning Engineer


`@script`
In previous video lecture you learned about sequence data types. We also explored real world examples of such dataset. In this video we are going to explore some of the technique that can be used to process such data and their limitations. Eventually, we will learn about state of the art neural network called Recurrent neural networks for sequence data processing.


---
## Sequence Data Processing Intuition

```yaml
type: "FullSlide"
key: "5f204109d5"
```

`@part1`
Best working example of state of art sequence data processing mechanism is human brain.

Human brain processes sequence data by

- Persisting previous information such as words, sentences, context.
- Given context and previous experience tries to predict foreseeable pattern.


`@script`
Many difficult and complex real world engineering problems can be solved using nature as inspiration. For example, first airplanes designed by Wright brothers took bird and their flight as example. Similarly, for sequence data processing we can also follow similar intuition of taking best working example from nature for base design. Best working example of state of art sequence data processing mechanism is human brain. Human brain processing sequence data by Persisting previous information such as words, sentences, context. Meaning human brain stores and understands each word before processing the next word. And it uses the previous word to understand the next word. Or in case of bigger problem, human brain tries to remember previous sentences or context to understand the whole context of the text or situation. Often times, human brain given context or previous experience tries to predict foreseeable pattern. And if the brain encounters different outcome than expected or predicted than it often gets shocked or surprised!


---
## Limitation of Convoluted Neural Network

```yaml
type: "FullSlide"
key: "2baf7bf0dd"
```

`@part1`
Sequence data processing with CNN and traditional neural networks has fundamental flaws.

1. CNN assumes all datapoint are independent and previous datapoint doesn't affect results of current datapoint.
2. Most CNN for Natural Language Processing problem, including state of the art models, contains filter with maximum size of 5 or 7.
3. Accepts fixed size inputs and produces fixed size output.

Point 2 tried to address limitation of 1 in certain way but point 1 still holds true despite the technique.


`@script`
Traditional neural network and convoluted neural networks have became state of the art for variety of applications such as image processing and sentiment analysis. Hence, it is natural to implement these as base model to evaluate performance on problems like Language modeling or other complex natural language processing task. However, these models have few fundamental flaws. One of the major assumption of CNN model is that it assumes all datapoint are independent and previous datapoint doesn't affect results of current datapoint. However, as we learned in earlier slides, for sequence data processing knowing previous state or context is very essential to predict the existing state. Hence, these models are not better suited for such problems. However, CNN did try to tackle the problem of knowing previous state using different size filter for convolution for extracting information/context. But this mechanism is limited to one single example only. And this can work great when it comes to problems like sentiment analysis, but in problems like generating text sequences, it would not be useful. Lastly, CNN and other traditional neural networks tend to accept fixed size input and produce fixed size output which not the case when it comes to sequence data.


---
## Introduction to Recurrent Neural Networks

```yaml
type: "FullSlide"
key: "2c953d9da1"
```

`@part1`
Recurrent neural network tries to emit the behavior of human brain by incorporating previous information into computation of existing state. Image below illustrates simple Recurrent neural network.

![RNN-Model](https://i.imgur.com/OqXYQX5.png)


`@script`
So to overcome the limitations of the traditional algorithms, we need to design a new algorithm that can address them in a meaningful and simplistic way. Recurrent neural network tries to emit the behavior of human brain by incorporating previous information into computation of existing state. Image below illustrates simple Recurrent neural network. So here we can see the network that has 4 different RNN cell and each cell has four different input and output ports. First side arrow provides RNN cell with information about previous state and the input arrow provides information about current state. Given these two pieces of information RNN cell would then try to compute an output denoted by top outgoing arrow of the RNN cell. It would also produce information about combination of existing and previous state that can be consumed by next RNN cell as memory to process its existing state of information. By such mechanism, we can have context or persistent of previous state for each processing unit and produce meaningful results just like our brain. In the example above RNN could be used as a way to translate simple text, the first RNN cell receives input word "I" and produces an output and then sends information to next cell about existing word I and previous words. Similarly next cell would receive input as "like" and produce output as well as memory based state that can be consumed by next RNN cell. Of course, this architecture would not solve complex problem like neural machine translation because it has fixed size input mapped to fixed size output. But it will provide you with good intuition about how RNN works and think in that mechanism for your own problem.


---
## Introduction to Recurrent Neural Network Continued

```yaml
type: "FullSlide"
key: "c1d67f508d"
```

`@part1`
In simple terms, Recurrent neural networks are chain of repeated (recurrent) small networks that perform similar operations. Hence they are called "Recurrent" neural networks. 

Recurrent neural networks can handle variable size input data and output data and persist information unlike traditional neural networks.

Because of the memory persisting nature of recurrent neural network, they are state of the art for variety of application in Natural Language processing such as 
- Language Modeling
- Generating Text
- Machine Translation
- Chatbots


`@script`
So in simple terms, Recurrent neural networks are chain of repeated (recurrent) small networks that perform similar operations. Hence they are called "Recurrent" neural networks. Because as you saw in previous slide it has similar RNN cells that were chained with each other, so they were repetitive or recurrent. Now these architecture can be tweaked to produced variable size input and variable size output. We will explore various types of recurrent neural network architecture in future. Now, let us talk about application of RNNs. Because of the memory persisting nature of recurrent neural network, they are state of the art for variety of application in Natural Language processing such as Language Modeling, Generating Text, Machine Translation, Chatbots and many more. Generally, applications of Recurrent neural network are mainly in Natural language processing applications.


---
## Simple RNN using Keras

```yaml
type: "FullSlide"
key: "8b3c2ba17d"
```

`@part1`
```
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import Embedding
from keras.layers import LSTM

max_features = 1024

model = Sequential()
model.add(Embedding(max_features, output_dim=256))
model.add(LSTM(128))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

model.fit(x_train, y_train, batch_size=16, epochs=10)
score = model.evaluate(x_test, y_test, batch_size=16)```


`@script`
Code for simple RNN model implementation.


---
## Conclusion

```yaml
type: "FinalSlide"
key: "4ce843e7d4"
```

`@script`
Conclusion and future topics

