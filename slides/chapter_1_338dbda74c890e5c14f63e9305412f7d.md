---
title: Insert title here
key: 338dbda74c890e5c14f63e9305412f7d

---
## Introduction to RNN and Basic Assumptions of various models

```yaml
type: "TitleSlide"
key: "00298cd64c"
```

`@lower_third`

name: Aakash Moghariya
title: Machine Learning Engineer


`@script`
In previous video lecture you learned about sequence data types. We also explored real world examples of such dataset. In this video we are going to explore some of the technique that can be used to process such data and their limitations. Eventually, we will learn about state of the art neural network called Recurrent neural networks for sequence data processing.


---
## Sequence Data Processing Intuition

```yaml
type: "FullSlide"
key: "5f204109d5"
```

`@part1`
Best working example of state of art sequence data processing mechanism is human brain.

Human brain processes sequence data by

- Persisting previous information such as words, sentences, context.
- Given context and previous experience tries to predict foreseeable pattern.


`@script`
Many difficult and complex real world engineering problems can be solved using nature as inspiration. For example, first airplanes designed by Wright brothers took bird and their flight as example. Similarly, for sequence data processing we can also follow similar intuition of taking best working example from nature for base design. Best working example of state of art sequence data processing mechanism is human brain. Human brain processing sequence data by Persisting previous information such as words, sentences, context. Meaning human brain stores and understands each word before processing the next word. And it uses the previous word to understand the next word. Or in case of bigger problem, human brain tries to remember previous sentences or context to understand the whole context of the text or situation. Often times, human brain given context or previous experience tries to predict foreseeable pattern. And if the brain encounters different outcome than expected or predicted than it often gets shocked or surprised!


---
## Limitation of Convoluted Neural Network

```yaml
type: "FullSlide"
key: "2baf7bf0dd"
```

`@part1`
Sequence data processing with CNN and traditional neural networks has fundamental flaws.

1. CNN assumes all datapoint are independent and previous datapoint doesn't affect results of current datapoint.
2. Most CNN for Natural Language Processing problem, including state of the art models, contains filter with maximum size of 5 or 7.
3. Accepts fixed size inputs and produces fixed size output.

Point 2 tried to address limitation of 1 in certain way but point 1 still holds true despite the technique.


`@script`
Teaches student limitations of traditional approaches such as convoluted neural network to process sequence data. It also states assumptions of CNN.


---
## Introduction to Recurrent Neural Networks

```yaml
type: "FullSlide"
key: "2c953d9da1"
```

`@part1`
Recurrent neural network tries to emit the behavior or human brain by incorporating previous information into computation of existing state. Image below illustrates simple Recurrent neural network.

![RNN-Model](https://i.imgur.com/OqXYQX5.png)


`@script`
Introduction to Recurrent Neural Networks


---
## Introduction to Recurrent Neural Network Continued

```yaml
type: "FullSlide"
key: "c1d67f508d"
```

`@part1`
In simple terms, Recurrent neural networks are chain of repeated (recurrent) small networks that perform similar operations. Hence they are called "Recurrent" neural networks. 

Recurrent neural networks can handle variable size input data and output data and persist information unlike traditional neural networks.

Because of the memory persisting nature of recurrent neural network, they are state of the art for variety of application in Natural Language processing such as 
- Language Modeling
- Generating Text
- Machine Translation
- Chatbots


`@script`
Define Recurrent in RNN and Applications of RNN.


---
## Simple RNN using Keras

```yaml
type: "FullSlide"
key: "8b3c2ba17d"
```

`@part1`
```
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import Embedding
from keras.layers import LSTM

max_features = 1024

model = Sequential()
model.add(Embedding(max_features, output_dim=256))
model.add(LSTM(128))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

model.fit(x_train, y_train, batch_size=16, epochs=10)
score = model.evaluate(x_test, y_test, batch_size=16)```


`@script`
Code for simple RNN model implementation.


---
## Conclusion

```yaml
type: "FinalSlide"
key: "4ce843e7d4"
```

`@script`
Conclusion and future topics

