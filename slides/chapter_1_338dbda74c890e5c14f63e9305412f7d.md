---
title: Insert title here
key: 338dbda74c890e5c14f63e9305412f7d
video_link:
  mp3: https://ruturajpatel.com/RNN_audio.mp3

---
## Introduction to RNN and Basic Assumptions of various models

```yaml
type: "TitleSlide"
key: "00298cd64c"
```

`@lower_third`

name: Aakash Moghariya
title: Machine Learning Engineer


`@script`
In previous chapter you learned about sequence data and saw real world examples such dataset. In this video we are going to explore traditional algorithms and their limitations for process such dataset. Eventually, we will learn about state of the art technique called Recurrent neural networks for sequence data processing.


---
## Sequence Data Processing Intuition

```yaml
type: "FullSlide"
key: "5f204109d5"
```

`@part1`
Best working example of state of art sequence data processing mechanism is human brain. {{1}}

Human brain processes sequence data by {{2}}

- Persisting previous information such as words, sentences, context. {{2}}
- Given context and previous experience tries to predict foreseeable pattern. {{2}}


`@script`
Many complex engineering problems can be solved by taking nature as inspiration. For example, first airplanes' design by Wright brothers took bird's flights as an intuition. Same process can be followed for our problem statement. Best working example of sequence data processing in nature is human brain. To process this type of data it stores previous information such as words or sentences to understand current word or sentence. Or in case of bigger problem, it stores previous context to understand given text or situation. Often times, human brain tries to predict foreseeable pattern given piece of information. And it is surprised or shocked if it encounters different outcome than what was expected!


---
## Limitation of Convoluted Neural Network

```yaml
type: "FullSlide"
key: "2baf7bf0dd"
```

`@part1`
Sequence data processing with CNN and traditional neural networks has fundamental flaws.{{1}}

1. CNN assumes all datapoint are independent of each other. {{2}}
2. Accepts fixed size inputs and produces fixed size output. {{2}}


`@script`
Traditional neural network and convoluted neural networks have became state of the art for variety of applications such as image processing and sentiment analysis. But these models have major flaws when it comes to processing sequence data. For example, CNN model assumes that all datapoint are independent of each other. Meaning results of previous data-points does not affect result of current datapoint. Clearly this is not the case for sequence data processing as we learned in earlier slides. For processing such dataset it is essential to know previous state or context as it affect the meaning of current state. Similarly another problem with CNN and other traditional neural networks have is that it accept fixed size input to produce fixed size output. And such pattern are rarely encountered in sequence dataset.


---
## Introduction to Recurrent Neural Networks

```yaml
type: "FullSlide"
key: "2c953d9da1"
```

`@part1`
Recurrent neural network tries to emit the behavior of human brain by incorporating previous information into computation of existing state. Image below illustrates simple Recurrent neural network. {{1}}

![RNN-Model](https://i.imgur.com/OqXYQX5.png) {{2}}


`@script`
To overcome limitations of the traditional algorithms for sequence data processing, a new algorithm called "Recurrent neural network" can be used. Just like human brain it incorporates previous information into understanding existing state. To do that RNN consists of mini neural network called cells that can be classified as standalone networks. Each cell takes previous state and an embeddings as an input to produce one or more piece of information as an output. RNN basically consists of N repetitive units of cell to produce a network. Image below illustrates simple RNN with 4 cell each having two input and two output port. Left side arrow provides RNN cell with information about previous state and the bottom provides embeddings. Given these two piece of information RNN cell would compute an output denoted by top outgoing arrow and a context denoted by right arrow. This context can then be used by chained RNN cell to perform similar computation and similar output. This process is repeated N times in RNN, where N is number of cells in RNN. By such mechanism, we can have context for each processing unit and produce meaningful results just like our brain. Architecture in the illustration can be used to solve problem like machine translation. Each cell receives embeddings of the word as an input as seen in the illustration to produce a translation. Of course, this simple architecture would not solve problem of machine translation fully, because it has fixed size input mapped to fixed size output. But it will provide you with good intuition about how RNN works and think in that mechanism for your own problem.


---
## Introduction to Recurrent Neural Network Continued

```yaml
type: "FullSlide"
key: "c1d67f508d"
```

`@part1`
In simple terms, Recurrent neural networks are chain of repeated (recurrent) small networks that perform similar operations. Hence they are called "Recurrent" neural networks. {{1}}

Recurrent neural networks can handle variable size input data and output data and persist information unlike traditional neural networks. {{2}}

Because of the memory persisting nature of recurrent neural network, they are state of the art for variety of application in Natural Language processing such as {{3}}
- Language Modeling {{3}}
- Generating Text {{3}}
- Machine Translation {{3}}
- Chatbots {{3}}


`@script`
So in summary, Recurrent neural networks are chain of "recurrent" cells or small networks that perform similar operations. Hence they are called "Recurrent" neural networks. Using these cells we can design custom RNN architecture having variable size input and variable size output where each cell has different size of input and output. We will explore various types of recurrent neural network architecture in future. For now, let us talk about application of RNNs. Because of the memory persisting nature of recurrent neural network, they are state of the art for variety of application in Natural Language processing domain such as Language Modeling, Generating Text, Machine Translation, Chatbots and many more. General applications of Recurrent neural network are mainly in Natural language processing domain.


---
## Simple RNN using Keras

```yaml
type: "FullSlide"
key: "8b3c2ba17d"
```

`@part1`
```
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import Embedding
from keras.layers import LSTM

max_features = 1024

model = Sequential()
model.add(Embedding(max_features, output_dim=256))
model.add(LSTM(128))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

model.fit(x_train, y_train, batch_size=16, epochs=10)
score = model.evaluate(x_test, y_test)```


`@script`
Now let's implement a simple Recurrent neural network using Keras. This example is taken from the Keras's documentation. The code implements a type of neural network called LSTM that we will explore in future. It is a very simple model that takes sequence data as input and produces 1 single output. This type of network can be used for application such as Sentiment Analysis. The network has one layer of recurrent units followed by dropout layer and dense layer. First, we initialize a variable to limit our input sequence size to 1024. After that we implement model architecture as it can be seen in the code. Sequential function in Keras implements a sequential model. One thing to note here is that sequential doesn't mean "Sequence" that we defined in previous lecture. It just means that our model architecture has layers stacked one after other. Another type of models architecture that we could have is layer skipping which will not go in detail. So just understand that "Sequential" in Keras has no relationship to the "Sequence" of sequence data that we defined. Following that we keep adding new layers to our model using model.add function of Keras. It takes input dimension and output dimension in our case max_feature size which is 1024 and output being 256. Then the model has 128 units of LSTM, much like 128 cells of RNN cell that we explored before. And then we have dropout layer that would randomly drop 50% of our all nodes in the result and then dense layer that would produce 1 single node as an output. This network could be used for application such as sentiment analysis! Then we compile our model using model.compile. This would create an architecture of our model which is implemented as a graph in tesorflow. And then we are fitting model on our dataset for 10 epochs. And batch_size is 16. Batch size is nothing by number of datapoint for which error would be computed and average before updates in weight happens. I am sure that you would have explored this parameter before in other classes so we will not go in detail on that. And lastly, we evaluate our model using model.evaluate.
Don't worry if you did not understand this code fully, we will be going into detail about each aspect in future and understand bits and pieces of this thoroughly. But I just wanted to give you an idea that implementing RNN can be extremely easy despite it seeming complicated.


---
## Conclusion

```yaml
type: "FinalSlide"
key: "4ce843e7d4"
```

`@script`
In summary, we learned limitations traditional neural network algorithms used to process sequence data. And then we intuitively derived a way to process sequence data using human brain as a working example. After that we saw how Recurrent neural network can be used to process sequence data and we finished off with a simple example of RNN implemented with Keras. Now let us explore how a forward pass of computation would work in Recurrent Neural Network.

